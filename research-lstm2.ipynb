{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 45533,
     "databundleVersionId": 5748852,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30446,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Flatten\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-03-10T19:10:22.717724Z",
     "iopub.execute_input": "2024-03-10T19:10:22.718133Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T02:18:15.116658Z",
     "start_time": "2024-04-09T02:18:05.269676Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "pd.options.mode.chained_assignment = None  # default='warn'\ntf.random.set_seed(42)\nnp.random.seed(42)\nkeras.backend.clear_session()",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T02:18:23.470692Z",
     "start_time": "2024-04-09T02:18:20.858774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rober\\Desktop\\Computer Science\\Data Mining\\dm_venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:73: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Reference: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/384359\n",
    "dtypes={\n",
    "    'elapsed_time':np.int32,\n",
    "    'event_name':'category',\n",
    "    'name':'category',\n",
    "    'level':np.uint8,\n",
    "    'room_coor_x':np.float32,\n",
    "    'room_coor_y':np.float32,\n",
    "    'screen_coor_x':np.float32,\n",
    "    'screen_coor_y':np.float32,\n",
    "    'hover_duration':np.float32,\n",
    "    'text':'category',\n",
    "    'fqid':'category',\n",
    "    'room_fqid':'category',\n",
    "    'text_fqid':'category',\n",
    "    'fullscreen':'category',\n",
    "    'hq':'category',\n",
    "    'music':'category',\n",
    "    'level_group':'category'}\n",
    "\n",
    "dataset_df = pd.read_csv('train.csv', dtype=dtypes)\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T02:35:56.621440Z",
     "start_time": "2024-04-09T02:33:20.953214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (26296946, 20)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "labels = pd.read_csv('train_labels.csv')\n",
    "labels['session'] = labels.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "labels['q'] = labels.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Categorizing Features on the Type\nCATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\nNUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y', \n        'screen_coor_x', 'screen_coor_y', 'hover_duration']",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Reference: https://www.kaggle.com/code/cdeotte/random-forest-baseline-0-664/notebook\ndef feature_engineer(dataset_df):\n    dfs = []\n    for c in CATEGORICAL:\n        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('nunique')\n        tmp.name = tmp.name + '_nunique'\n        dfs.append(tmp)\n    for c in NUMERICAL:\n        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('mean')\n        dfs.append(tmp)\n    for c in NUMERICAL:\n        tmp = dataset_df.groupby(['session_id','level_group'])[c].agg('std')\n        tmp.name = tmp.name + '_std'\n        dfs.append(tmp)\n    dataset_df = pd.concat(dfs,axis=1)\n    dataset_df = dataset_df.fillna(-1)\n    dataset_df = dataset_df.reset_index()\n    dataset_df = dataset_df.set_index('session_id')\n    return dataset_df",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "dataset_df = feature_engineer(dataset_df)\nprint(\"Dataset shape is {}\".format(dataset_df.shape))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Splitting DataSet into Train and Valid\ndef split_dataset(dataset, test_ratio=0.20):\n    USER_LIST = dataset.index.unique()\n    split = int(len(USER_LIST) * (1 - 0.20))\n    return dataset.loc[USER_LIST[:split]], dataset.loc[USER_LIST[split:]]\n\n# Generating Split DataSet\ntrain_x, test_x = split_dataset(dataset_df)\nprint(\"{} examples in training, {} examples in validation.\".format(\n    len(train_x), len(test_x)))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Splitting the Data into Train and Test\ntrain_x, valid_x = split_dataset(train_x)\nprint(\"{} examples in training, {} examples in testing.\".format(\n    len(train_x), len(valid_x)))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Function to Split DataSet into Parts\ndef save_to_multiple_csv_files(data, name_prefix, question, header=None, n_parts=5):\n    # Setting the Directory\n    # Creating Directory for Each Question\n    game_prediction_dir = os.path.join(\"/kaggle/working/datasets_\"+str(question), \"student_performance_data\")\n    os.makedirs(game_prediction_dir, exist_ok=True)\n    path_format = os.path.join(game_prediction_dir, \"my_{}_{:02d}.csv\")\n\n    filepaths = []\n    m = len(data)\n    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n        part_csv = path_format.format(name_prefix, file_idx)\n        filepaths.append(part_csv)\n        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n            if header is not None:\n                f.write(header)\n                f.write(\"\\n\")\n            for row_idx in row_indices:\n                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n                f.write(\"\\n\")\n    return filepaths",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generating CSV files for Each Question and Saving them\ntrain_file_paths_for_questions = []\nvalid_file_paths_for_questions = []\ntest_file_paths_for_questions = []\n\nheader_cols = train_x.columns\nheader = \",\".join(header_cols)\n\nfor q_no in range(1,19):\n    \n    # Selecting the Group based on Question Number\n    if q_no<=3: grp = '0-4'\n    elif q_no<=13: grp = '5-12'\n    elif q_no<=22: grp = '13-22'\n    print(\"##### Generating CSV for q_no\", q_no, \"grp\", grp)\n    \n    # Filter the rows in the datasets based on the selected level group. \n    train_df = train_x.loc[train_x.level_group == grp]\n    train_users = train_df.index.values\n    valid_df = valid_x.loc[valid_x.level_group == grp]\n    valid_users = valid_df.index.values\n    test_df = test_x.loc[test_x.level_group == grp]\n    test_users = test_df.index.values\n    \n    # Select the labels for the related q_no.\n    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n    test_labels = labels.loc[labels.q==q_no].set_index('session').loc[test_users]\n    \n     # Add the label to the filtered datasets.\n    train_df[\"correct\"] = train_labels[\"correct\"]\n    valid_df[\"correct\"] = valid_labels[\"correct\"]\n    test_df[\"correct\"] = test_labels[\"correct\"]\n    \n    # Dropping Column Level Group\n    train_ds_data = train_df.drop(columns=['level_group'])\n    valid_ds_data = valid_df.drop(columns=['level_group'])\n    test_ds_data = test_df.drop(columns=['level_group'])\n    train_ds_data.reset_index()\n    valid_ds_data.reset_index()\n    test_ds_data.reset_index()\n    \n    # Calling function to generate CSVs\n    train_filepaths = save_to_multiple_csv_files(train_ds_data.to_numpy(), \"train\", \"q_no_\"+str(q_no), header, n_parts=5)\n    valid_filepaths = save_to_multiple_csv_files(valid_ds_data.to_numpy(), \"valid\", \"q_no_\"+str(q_no), header, n_parts=5)\n    test_filepaths = save_to_multiple_csv_files(test_ds_data.to_numpy(), \"test\", \"q_no_\"+str(q_no), header, n_parts=5)\n    \n    # Saving File Paths\n    train_file_paths_for_questions.append(train_filepaths)\n    valid_file_paths_for_questions.append(valid_filepaths)\n    test_file_paths_for_questions.append(test_filepaths)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Pre Process Function\nn_inputs = 21\ndef preprocess(line):\n\n    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    X = tf.stack(fields[:-1])\n    y = tf.stack(fields[-1:])\n    return X, y",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# CSV Reader for Train\ndef csv_reader_dataset(filepaths, repeat=1, n_readers=5,  # number of files or filepaths\n                       n_read_threads=None, shuffle_buffer_size=10000,\n                       n_parse_threads=5, batch_size=32):\n    \n    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n    dataset = dataset.interleave(\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip the header row via map_func\n        cycle_length=n_readers, # 'interleave' pull cycle_length(=n_readers) file paths(1 by 1) from the 'dataset'\n        num_parallel_calls=n_read_threads) \n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    \n    return dataset.prefetch(1)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Using the Saved CSV Loading the Data and saving them to a List\ntrain_set_list = []\nvalid_set_list = []\ntest_set_list = []\n\n\nfor q_no in range(1,19):\n\n    # Select level group for the question based on the q_no.\n    if q_no<=3: grp = '0-4'\n    elif q_no<=13: grp = '5-12'\n    elif q_no<=22: grp = '13-22'\n    print(\"##### Loading CSV for q_no\", q_no, \"grp\", grp)\n    \n    train_set = csv_reader_dataset(train_file_paths_for_questions[q_no - 1])\n    valid_set = csv_reader_dataset(valid_file_paths_for_questions[q_no - 1]) \n    test_set = csv_reader_dataset(test_file_paths_for_questions[q_no - 1])   \n    \n    train_set_list.append(train_set)\n    valid_set_list.append(valid_set)\n    test_set_list.append(test_set)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training Models for Each Question\ntest_loss_and_accuracy_list = []\nhistory_models = []\nmodels = {}\nf1_score_list = []\n\nfor q_no in range(1,19):\n\n    # Select level group for the question based on the q_no.\n    if q_no<=3: grp = '0-4'\n    elif q_no<=13: grp = '5-12'\n    elif q_no<=22: grp = '13-22'\n    print(\"##### Training for q_no\", q_no, \"grp\", grp)\n    \n    train_set = train_set_list[q_no - 1]\n    valid_set = valid_set_list[q_no - 1]\n    test_set = test_set_list[q_no - 1]\n    \n    \n    model = keras.models.Sequential([\n        keras.layers.LSTM(20, return_sequences=True, input_shape=(21, 1)),\n        keras.layers.LSTM(20, return_sequences=True),\n        keras.layers.TimeDistributed(keras.layers.Dense(1))\n    ])\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', patience=3,  restore_best_weights=True)\n    history = model.fit(train_set, epochs=1, validation_data=(valid_set), callbacks=[early_stopping])\n    \n    # Store the model\n    models[f'{grp}_{q_no}'] = model    \n    \n    # Saving Accuracies\n    results = model.evaluate(test_set)\n    test_loss_and_accuracy_list.append(results)\n    \n    # Saving History of Models\n    history_models.append(history)\n    \n    # F1 Score\n    y_true_numpy_list = []\n    \n    predictions = model.predict(test_set, verbose=0)[:, -1][..., np.newaxis]\n    predictions = predictions.round().astype(int).flatten()\n    \n    for x_batch, y_batch in test_set:\n    \n        y_batch = y_batch.numpy()\n        y_batch = y_batch.round().astype(int).flatten()\n\n        y_true_numpy_list.append(y_batch)\n        \n    y_true_numpy = np.concatenate(y_true_numpy_list)\n    f1_score_list.append(f1_score(y_true_numpy, predictions, average='weighted'))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import jo_wilder\nenv = jo_wilder.make_env()\niter_test = env.iter_test()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "best_threshold = 0.63\n\nlimits = {'0-4':(1,4), '5-12':(4,14), '13-22':(14,19)}\n\nfor (test, sample_submission) in iter_test:\n    \n    # FEATURE ENGINEER TEST DATA\n    df = feature_engineer(test)\n    \n    # INFER TEST DATA\n    grp = test.level_group.values[0]\n    a, b = limits[grp]\n    for t in range(a, b):\n        clf = models[f'{grp}_{t}']\n        \n        # Filter the columns of df based on the condition\n        p = df.loc[:, df.columns != 'level_group']\n        \n        # Make predictions using the model\n        predictions = clf.predict(p)  # Use clf instead of model\n        \n        # Create a mask to select the relevant rows in sample_submission\n        mask = sample_submission.session_id.str.contains(f'q{t}')\n        \n        # Update the 'correct' column in sample_submission\n        n_predictions = (predictions > best_threshold).astype(int)\n        sample_submission.loc[mask, 'correct'] = n_predictions.flatten()[0]\n    \n    env.predict(sample_submission[['session_id', 'correct']])",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "! cat submission.csv",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
